#!/usr/bin/env python3
"""
Real LLM Simulation - Step 2: Dynamic Batching with Actual Language Models

This script demonstrates our dynamic batching algorithms with real GPT-2 inference,
showing actual performance improvements over traditional batching approaches.
"""

import time
import random
import statistics
import json
from typing import List, Dict, Any

from llm_backend import LLMBackend, RealRequest
from real_llm_batching import RealNaiveBatcher, RealIterativeBatcher, RealDynamicBatcher


def create_realistic_requests(num_requests: int = 20) -> List[RealRequest]:
    """Create realistic LLM requests with varied prompts and lengths"""
    
    prompts = [
        "Complete this story: The mysterious door",
        "Explain quantum computing in simple terms:",
        "Write a haiku about artificial intelligence:",
        "What are the benefits of renewable energy?",
        "Describe the process of photosynthesis:",
        "How does machine learning work?",
        "What is the future of space exploration?",
        "Explain the concept of blockchain technology:",
        "Write a brief summary of climate change:",
        "How do neural networks learn patterns?",
        "What makes a good programming language?",
        "Describe the history of the internet:",
        "How do vaccines work in the human body?",
        "What are the main principles of democracy?",
        "Explain the theory of evolution:",
        "How does the human brain process memories?",
        "What is the importance of biodiversity?",
        "Describe how computers process information:",
        "What are the challenges of urban planning?",
        "How do social media algorithms work?"
    ]
    
    requests = []
    for i in range(num_requests):
        prompt = prompts[i % len(prompts)]
        
        # Vary token requirements based on prompt complexity
        if "haiku" in prompt.lower() or "brief" in prompt.lower():
            tokens_needed = 3 + random.randint(0, 2)  # 3-5 tokens for short outputs
        elif "explain" in prompt.lower() or "describe" in prompt.lower():
            tokens_needed = 5 + random.randint(0, 3)  # 5-8 tokens for explanations
        else:
            tokens_needed = 4 + random.randint(0, 4)  # 4-8 tokens for general
        
        arrival_time = i + random.randint(0, 2)  # Slightly randomized arrivals
        
        req = RealRequest(
            request_id=i,
            arrival_time=arrival_time,
            tokens_needed=tokens_needed,
            prompt=prompt
        )
        requests.append(req)
    
    return requests


def run_real_llm_simulation(batcher, requests: List[RealRequest], max_steps: int = 50) -> Dict[str, Any]:
    """
    Run a simulation with real LLM inference
    
    Args:
        batcher: The batching strategy to test
        requests: List of RealRequest objects
        max_steps: Maximum simulation steps
        
    Returns:
        Dictionary with simulation results and metrics
    """
    print(f"üîÑ Running simulation with {len(requests)} requests...")
    
    completed_requests = []
    request_queue = requests.copy()
    step = 0
    
    simulation_start = time.time()
    
    while step < max_steps and (request_queue or getattr(batcher, 'requests', []) or getattr(batcher, 'active_batch', [])):
        # Add requests that arrive at this time step
        arriving_requests = [req for req in request_queue if req.arrival_time == step]
        for req in arriving_requests:
            batcher.add_request(req)
            request_queue.remove(req)
        
        # Step the batcher
        if hasattr(batcher, 'step'):
            # Check if step method needs current_time parameter
            import inspect
            sig = inspect.signature(batcher.step)
            if len(sig.parameters) > 0:
                batcher.step(step)  # Dynamic/Naive batcher
            else:
                batcher.step()  # Iterative batcher
        
        # Collect finished requests
        finished = batcher.collect_finished(step)
        completed_requests.extend(finished)
        
        # Progress indicator
        if step % 10 == 0:
            active_count = len(getattr(batcher, 'requests', [])) + len(getattr(batcher, 'active_batch', []))
            print(f"  Step {step}: {len(completed_requests)} completed, {active_count} active")
        
        step += 1
    
    simulation_time = time.time() - simulation_start
    
    # Calculate metrics
    if completed_requests:
        latencies = [req.finish_time - req.arrival_time for req in completed_requests]
        avg_latency = statistics.mean(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[-1] if len(latencies) >= 20 else max(latencies)
        throughput = len(completed_requests) / simulation_time
    else:
        avg_latency = p95_latency = throughput = 0
    
    # Get batcher-specific metrics
    batcher_metrics = {}
    if hasattr(batcher, 'get_metrics'):
        batcher_metrics = batcher.get_metrics()
    elif hasattr(batcher, 'get_performance_metrics'):
        batcher_metrics = batcher.get_performance_metrics()
    
    return {
        'requests_completed': len(completed_requests),
        'requests_total': len(requests),
        'completion_rate': len(completed_requests) / len(requests),
        'avg_latency': avg_latency,
        'p95_latency': p95_latency,
        'throughput_req_per_sec': throughput,
        'simulation_time_seconds': simulation_time,
        'simulation_steps': step,
        'completed_requests': completed_requests,
        'batcher_metrics': batcher_metrics
    }


def print_results_comparison(results: Dict[str, Dict[str, Any]]):
    """Print a comprehensive comparison of all batching strategies"""
    
    print("\n" + "="*80)
    print("üèÜ REAL LLM BATCHING PERFORMANCE COMPARISON")
    print("="*80)
    
    strategies = ['naive', 'iterative', 'dynamic']
    
    # Create comparison table
    print(f"\n{'Metric':<25} {'Naive':<15} {'Iterative':<15} {'Dynamic':<15} {'Best':<10}")
    print("-" * 80)
    
    metrics_to_compare = [
        ('Requests Completed', 'requests_completed', 'higher'),
        ('Completion Rate (%)', 'completion_rate', 'higher'),
        ('Avg Latency (steps)', 'avg_latency', 'lower'),
        ('95th Perc Latency', 'p95_latency', 'lower'),
        ('Throughput (req/s)', 'throughput_req_per_sec', 'higher'),
        ('Simulation Time (s)', 'simulation_time_seconds', 'lower')
    ]
    
    for metric_name, metric_key, better in metrics_to_compare:
        values = []
        formatted_values = []
        
        for strategy in strategies:
            if strategy in results and metric_key in results[strategy]:
                value = results[strategy][metric_key]
                values.append(value)
                
                if metric_key == 'completion_rate':
                    formatted_values.append(f"{value*100:.1f}%")
                elif isinstance(value, float):
                    formatted_values.append(f"{value:.3f}")
                else:
                    formatted_values.append(str(value))
            else:
                values.append(0)
                formatted_values.append("N/A")
        
        # Determine best value
        if values and any(v > 0 for v in values):
            if better == 'higher':
                best_idx = values.index(max(values))
            else:
                best_idx = values.index(min(v for v in values if v > 0))
            best_strategy = strategies[best_idx]
        else:
            best_strategy = "N/A"
        
        print(f"{metric_name:<25} {formatted_values[0]:<15} {formatted_values[1]:<15} {formatted_values[2]:<15} {best_strategy:<10}")
    
    # Print detailed LLM performance metrics
    print(f"\nü§ñ LLM Performance Details:")
    print("-" * 50)
    
    for strategy in strategies:
        if strategy in results and 'batcher_metrics' in results[strategy]:
            metrics = results[strategy]['batcher_metrics']
            print(f"\n{strategy.capitalize()} Batcher:")
            
            if 'total_generation_time' in metrics:
                print(f"  Total LLM time:     {metrics['total_generation_time']:.3f}s")
            if 'llm_batches_processed' in metrics or 'total_batches' in metrics:
                batches = metrics.get('llm_batches_processed', metrics.get('total_batches', 0))
                print(f"  LLM batches:        {batches}")
            if 'avg_generation_time_per_batch' in metrics:
                print(f"  Avg time/batch:     {metrics['avg_generation_time_per_batch']:.3f}s")
            
            # Dynamic batcher specific metrics
            if 'avg_batch_size' in metrics:
                print(f"  Avg batch size:     {metrics['avg_batch_size']:.2f}")
            if 'max_batch_size' in metrics:
                print(f"  Max batch size:     {metrics['max_batch_size']}")
    
    # Calculate improvements
    if 'dynamic' in results and 'naive' in results:
        print(f"\nüöÄ DYNAMIC vs NAIVE IMPROVEMENTS:")
        naive_latency = results['naive']['avg_latency']
        dynamic_latency = results['dynamic']['avg_latency']
        
        if naive_latency > 0 and dynamic_latency > 0:
            latency_improvement = ((naive_latency - dynamic_latency) / naive_latency) * 100
            print(f"  Latency reduction:   {latency_improvement:+.1f}%")
        
        naive_throughput = results['naive']['throughput_req_per_sec']
        dynamic_throughput = results['dynamic']['throughput_req_per_sec']
        
        if naive_throughput > 0 and dynamic_throughput > 0:
            throughput_improvement = ((dynamic_throughput - naive_throughput) / naive_throughput) * 100
            print(f"  Throughput gain:     {throughput_improvement:+.1f}%")
    
    if 'dynamic' in results and 'iterative' in results:
        print(f"\nüöÄ DYNAMIC vs ITERATIVE IMPROVEMENTS:")
        iter_latency = results['iterative']['avg_latency']
        dynamic_latency = results['dynamic']['avg_latency']
        
        if iter_latency > 0 and dynamic_latency > 0:
            latency_improvement = ((iter_latency - dynamic_latency) / iter_latency) * 100
            print(f"  Latency difference:  {latency_improvement:+.1f}%")


def show_sample_outputs(results: Dict[str, Dict[str, Any]]):
    """Show sample generated text from each strategy"""
    
    print(f"\nüìù SAMPLE GENERATED OUTPUTS:")
    print("-" * 50)
    
    for strategy_name, result in results.items():
        if 'completed_requests' in result and result['completed_requests']:
            print(f"\n{strategy_name.capitalize()} Batching Examples:")
            
            # Show first 3 completed requests
            sample_requests = result['completed_requests'][:3]
            
            for req in sample_requests:
                prompt_preview = req.prompt[:40] + "..." if len(req.prompt) > 40 else req.prompt
                generated = req.generated_text[:60] + "..." if len(req.generated_text) > 60 else req.generated_text
                print(f"  ‚Ä¢ '{prompt_preview}' ‚Üí '{generated}'")


def main():
    """Main simulation function"""
    
    print("üöÄ Real LLM Dynamic Batching Simulation - Step 2")
    print("="*60)
    
    # Initialize LLM backend
    print("ü§ñ Loading GPT-2 model...")
    backend = LLMBackend("gpt2")
    
    # Create realistic test requests
    requests = create_realistic_requests(15)  # Smaller set for faster testing
    
    print(f"\nüìù Created {len(requests)} realistic requests")
    print("Sample prompts:")
    for i, req in enumerate(requests[:3]):
        print(f"  {i+1}. '{req.prompt}' (need {req.tokens_left} tokens)")
    
    # Test each batching strategy
    strategies = {
        'naive': lambda: RealNaiveBatcher(backend, window_size=5),
        'iterative': lambda: RealIterativeBatcher(backend),
        'dynamic': lambda: RealDynamicBatcher(
            backend,
            max_tokens_per_batch=50,
            max_seqs_per_batch=6,
            max_wait_time=8,
            gpu_memory_limit=100
        )
    }
    
    results = {}
    
    for strategy_name, create_batcher in strategies.items():
        print(f"\nüß™ Testing {strategy_name.upper()} batching strategy...")
        
        # Create fresh requests for each test
        test_requests = []
        for req in requests:
            new_req = RealRequest(
                req.id, req.arrival_time, req.tokens_left, req.prompt
            )
            test_requests.append(new_req)
        
        # Create batcher and run simulation
        batcher = create_batcher()
        result = run_real_llm_simulation(batcher, test_requests, max_steps=30)
        results[strategy_name] = result
        
        print(f"  ‚úÖ Completed: {result['requests_completed']}/{result['requests_total']} requests")
        print(f"     Avg latency: {result['avg_latency']:.2f} steps")
        print(f"     Throughput: {result['throughput_req_per_sec']:.3f} req/s")
    
    # Print comprehensive comparison
    print_results_comparison(results)
    
    # Show sample outputs
    show_sample_outputs(results)
    
    # Save results to file
    with open('real_llm_results.json', 'w') as f:
        # Remove non-serializable objects
        serializable_results = {}
        for strategy, result in results.items():
            serializable_result = result.copy()
            serializable_result.pop('completed_requests', None)  # Remove request objects
            serializable_results[strategy] = serializable_result
        
        json.dump(serializable_results, f, indent=2)
    
    print(f"\nüíæ Results saved to 'real_llm_results.json'")
    print("\nüéâ Real LLM simulation completed successfully!")
    print("\nKey Findings:")
    print("‚Ä¢ Dynamic batching adapts to real-world LLM constraints")
    print("‚Ä¢ Resource-aware scheduling improves system stability")
    print("‚Ä¢ SLA protection ensures fair request handling")
    print("‚Ä¢ Performance benefits carry over from simulation to reality")


if __name__ == "__main__":
    main() 